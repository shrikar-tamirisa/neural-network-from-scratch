# neural-network-from-scratch

The goal of this project is to explore the mathematics, especially the optimization techniques utilized in designing neural networks. While several optimizers like Adam, RMSprop, SGD, Nadam, Adagrad, etc. are available, this project focusses on using Gradient Descent for backpropagation. Keras API, Tensorflow (open-source software library for machine learning and artificial intelligence) or PyTorch are popularly used for designing neural networks with minimal code. But these modules and libraries hide the mathematics and provide a simple frontend for users to work with. So, in this project neural networks will be implemented from scratch using vanilla Python, except for NumPy (to speed up the programs with vectorization) and Pandas (to import large datasets).
